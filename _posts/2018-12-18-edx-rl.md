---
layout: default
title:  "Reinforcement Learning Learning Notes"
date:    2018-12-18 10:57:02 -0700
categories: www
permalink: "/rl/"
---

## Introduction
The fundermental idea of reinforcement learning is from Thorndike's cat puzzle box experiment, yes, another cat in the box.
> Behaviour changes because of consequences.

Two type of RL _Tasks_: 
episodic such as tic tac toe that has and terminate state, 
or continuing one, such as smart thermometer that continuously runs to tune the temperature to keep peeps happy or irrate when set it wrong.

RL challenges: 
* Representation
* Generalization
* Exploration
* Temporal credit assignment 

> The key to artificial intelligence has always been the representation. - _Jeff Hawkins_


## Exploration
The "explore-exploit", the princle of "optimism in the face of uncertainty"

The smarter A/B testing could fit in here with Multi-Armed Bandit Framework. Or an application of drug trial.

Algorithms:
* round robin
* greedy 
* $$\{epsilon|$$ greedy
* Upper Confidence Bound (UCB)   
* Contextual Bandit LinUCB

Regret, is to quantify the loss or "price of information", Another form of $$y-{hat}y$$

Greedy and $$\{epsilon}$$-greedy has linear regret Lt >= Const * T
_insert UCB log regret here_

Bayesian UCB ? 


### discounted return 
$$\{gamma}$$

### markov decision process

### value function and policy
Value function, function of state, or state-action pairs.
Value function can be learnt from experience. 
_insert bellman equation_
Policy $${pi}$$, a mapping from state to probability of selection each available action

## Dynamnic Programming

* use value function to organize and struture the search for good policy
* turn Bellman equations into iterative updates

## Model-free learning

* Monte Carlo Learning
* Temporal Difference Learning

## Representation and Generalization - Function Approximation

trade off between sample efficient learning vs generality

* Linear function approximators
* Deep Q-Learning (DQN)
* Double DQN


## Policy Gradient

Doesn't care about value functions, DP or models, only cares about finding $${theta}$$ that maximizes rewards.

## Further Readings

* Thorndike’s puzzle boxes and the origins of the experimental analysis
of behavior (Chance, 1999):
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1284753/pdf/jeabehav
007200300433.pdf
* Animal intelligence : an experimental study of the associative
processes in animals (Thorndike, 1898):
https://archive.org/details/animalintelligen00thoruoft

*  UCB: https://jeremykun.com/2013/10/28/optimism-in-the-face-of-uncertainty-the-ucb1-algorithm/
*  Thompson sampling: https://dataorigami.net/blogs/napkin-folding/79031811-multi-armed-bandits
*  Finite-time Analysis of the Multi-armed Bandit Problem, Auer et al
http://dl.acm.org/citation.cfm?id=599677
*  An Empirical Evaluation of Thompson Sampling, Chapelle and Li https://papers.nips.cc/paper/4321-
an-empirical-evaluation-of-thompson-sampling
*  Tutorial, Dave Silver http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching_files/XX.pdf


* Decision Service http://ds.microsoft.com
*  Tutorial: http://hunch.net/~exploration_learning/
*  http://www.stat.berkeley.edu/~bartlett/courses/2014fall-cs294stat260/readings.html
*  LinUCB: https://arxiv.org/pdf/1003.0146.pdf


* Reinforcement Learning – An Introduction (Sutton and Barto, 2017):
*  http://www.incompleteideas.net/sutton/book/the-book-2nd.html
*  Part II: approximate solution methods + references
* Algorithms for Reinforcement Learning (Szepesvári, 2010). Synthesis
Lectures on Artificial Intelligence and Machine Learning

* Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M.
G., … Hassabis, D. (2015). Human-level control through deep
reinforcement learning. Nature, 518(7540), 529–533.
*  Introduce DQN, breakthrough result on learning to play Atari from pixels
* Riedmiller, M. (2005). Neural Fitted Q Iteration – First Experiences with
a Data Efficient Neural Reinforcement Learning Method. ECML 2005,
317-328.
*  Propose use of neural nets for Q-learning, Rprop optimizer – promising results


*  van Hasselt, H., Guez, A., & Silver, D. (2016). Deep Reinforcement
Learning with Double Q-learning. AAAI 2016, 2094-2100
http://arxiv.org/abs/1509.06461
* Schaul, T., Quan, J., Antonoglou, I., Silver, D. (2016). Prioritized
Experience Replay. ICLR 2016
https://arxiv.org/abs/1511.05952
* Arulkumaran, K., Deisenroth, M. P., Brundage, M., & Bharath, A. A.
(2017). A Brief Survey of Deep Reinforcement Learning. IEEE SPM
Special Issue on Deep Learning for Visual Understanding
http://arxiv.org/abs/1708.05866


* Pieter Abbeel's lecture on PGAC https://sites.google.com/view/deep-rl-bootcamp/lectures
* Andrej Karpathy's blogpost Pong from Pixels http://karpathy.github.io/2016/05/31/rl/
* Jan Peter's article http://www.scholarpedia.org/article/Policy_gradient_methods
* Simple statistical gradient-following algorithms for connectionist reinforcement learning by Williams, 1992
