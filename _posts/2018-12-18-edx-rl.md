---
layout: default
title:  "Reinforcement Learning Learning Notes"
date:    2018-12-18 10:57:02 -0700
categories: www
---

## Introduction
The fundermental idea of reinforcement learning is from Thorndike's cat puzzle box experiment, yes, another cat in the box.
> Behaviour changes because of consequences.

Two type of RL _Tasks_: 
episodic such as tic tac toe that has and terminate state, 
or continuing one, such as smart thermometer that continuously runs to tune the temperature to keep peeps happy or irrate when set it wrong.

RL challenges: 
* Representation
* Generalization
* Exploration
* Temporal credit assignment 

> The key to artificial intelligence has always been the representation. - _Jeff Hawkins_


## Exploration
The "explore-exploit", the princle of "optimism in the face of uncertainty"

The smarter A/B testing could fit in here with Multi-Armed Bandit Framework. Or an application of drug trial.

Algorithms:
* round robin
* greedy 
* $$\{epsilon|$$ greedy
* Upper Confidence Bound (UCB)   
* Contextual Bandit LinUCB

Regret, is to quantify the loss or "price of information", Another form of $$y-{hat}y$$

Greedy and $$\{epsilon}$$-greedy has linear regret Lt >= Const * T
_insert UCB log regret here_

Bayesian UCB ? 

elt, H., Guez, A., & Silver, D. (2016). Deep Reinforcement
Learning with Double Q-learning. AAAI 2016, 2094-2100
http://arxiv.org/abs/1509.06461
chaul, T., Quan, J., Antonoglou, I., Silver, D. (2016). Prioritized
Experience Replay. ICLR 2016
https://arxiv.org/abs/1511.05952
Î

### discounted return 
$$\{gamma}$$

### markov decision process

### value function and policy
Value function, function of state, or state-action pairs.
Value function can be learnt from experience. 
_insert bellman equation_
Policy $${pi}$$, a mapping from state to probability of selection each available action

## Dynamnic Programming

* use value function to organize and struture the search for good policy
* turn Bellman equations into iterative updates

## Model-free learning

* Monte Carlo Learning
* Temporal Difference Learning

## Representation and Generalization - Function Approximation

trade off between sample efficient learning vs generality

* Linear function approximators
* Deep Q-Learning (DQN)
* Double DQN


## Policy Gradient

Doesn't care about value functions, DP or models, only cares about finding $${theta}$$ that maximizes rewards.

## Further Readings

Thorndike puzzle boxes and the origins of the experimental analysis
of behavior (Chance, 1999):
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1284753/pdf/jeabehav
007200300433.pdf
Înimal intelligence : an experimental study of the associative
processes in animals (Thorndike, 1898):
https://archive.org/details/animalintelligen00thoruoft

B: https://jeremykun.com/2013/10/28/optimism-in-the-face-of-uncertainty-the-ucb1-algorithm/
Î
mpson sampling: https://dataorigami.net/blogs/napkin-folding/79031811-multi-armed-bandits
te-time Analysis of the Multi-armed Bandit Problem, Auer et al
http://dl.acm.org/citation.cfm?id=599677
ÎÎn Empirical Evaluation of Thompson Sampling, Chapelle and Li https://papers.nips.cc/paper/4321-
an-empirical-evaluation-of-thompson-sampling
il, Dave Silver http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching_files/XX.pdf


Decision Service http://ds.microsoft.com
il: http://hunch.net/~exploration_learning/
nUCB: https://arxiv.org/pdf/1003.0146.pdf


Reinforcement Learning ÎÎÎn Introduction (Sutton and Barto, 2017):
ÎÎrt II: approximate solution methods + references
ÎÎlgorithms for Reinforcement Learning (Szepesvári, 2010). Synthesis
Lectures on Artificial Intelligence and Machine Learning

Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M.
G., Î
