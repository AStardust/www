---
title: “Some notes on nnet”
date: 2019-10-30 21:45:30 -0700
---

nnet experiment 

Eventually it is just
Y = W*X + b + E(X)

all below just come in to minimize the error 

I - optimizer
First order Adam, SGD, RMSprop
Second order L-BFGS

II - loss = f(y-yhat) 
Cross entropy
L1 MAE, L2 MSE

III - hyperparameter search (tuning) 
Learning rate (lr) 
Architecture: Number of layers, type of layers, activation function, number of perceptrons 

====
toy perceptron
the idea of nn is quite simple and straightforward taking the input, excite neurons and produce adenosine then it comes down to reduce noises (optimization) and the speed (optimization again for network converge) most of the business problems does not require CNN or RNN rather solved by single or double hidden layer of vanilla neural networks

y = wx + b + E

train a model from iris, to keep it simple, two class prediction (first 100 obs.) Species ~ Sepal.Length + Sepal.Width



    p <- tanh(x %*% w + b)
                                                                 
    prob  = ifelse(p>0, p, 1+p)                              
    prob  = pmin(pmax(prob, eps), 1-eps)              
    y.hat = sign(p)                                       
  
    eta   <- y - y.hat                                   
    
    # ... this sucker DOES work 
    w <- w + t(x) %*% eta * alpha
    b <- b + sum(alpha * eta) 
    

